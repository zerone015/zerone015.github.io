---
title: "CPU Cache"
date: 2026-01-22 20:00:00 +0900
categories: [CS]
tags: [cache, mesi, false-sharing]
math: true
---


## What is CPU Cache?
---
CPU 내부에 있는 SRAM을 말한다. 메인 메모리(DRAM)는 CPU 연산 속도에 비해 너무 느리기 때문에 데이터 병목 현상이 발생한다. 이 병목을 완화하기 위해 CPU와 메인 메모리 사이에 SRAM이라는 작고 빠른 캐시를 두게 되었다.

### Locality of Reference
캐시는 지역성의 원리를 이용해 동작한다.
* **Temporal Locality:** 최근에 사용된 데이터는 가까운 미래에 재사용될 가능성이 높다.
* **Spatial Locality:** 접근한 데이터의 근처 데이터는 가까운 미래에 사용될 가능성이 높다.

### Cache Line
메인 메모리에서 캐시로 데이터를 가져올 때, 공간 지역성을 활용하기 위해 접근한 데이터 단위가 아닌 더 큰 단위로 가져온다. 이를 캐시라인이라 하며 대부분의 시스템에서 64B 크기를 갖는다. 캐시라인은 캐시가 데이터를 관리하는 최소 단위이다.

### Cache Hierarchy
현대 시스템은 더 빠른 성능을 위해 여러 단계의 캐시 계층을 갖는다. (L1, L2, L3 캐시) CPU 코어와 가까울수록 속도가 빠르고 용량이 작다.


## Mapping Strategies
---
물리 메모리 주소는 Tag, Index, Offset 세 부분으로 나뉘며, 이를 통해 데이터가 어느 캐시라인에 있는지 찾는다. 각 캐시라인에는 데이터 외에도 태그 값과 Valid bit 등의 메타데이터가 저장된다.

### 1. Direct Mapped Cache
물리 메모리의 특정 주소 데이터는 정해진 캐시라인에만 들어갈 수 있다.
* **Characteristics:** 탐색할 필요가 없어 빠르지만, 위치가 고정되어 있어 동일한 인덱스를 갖는 메모리 주소들이 부딪히는 충돌 미스가 잦다.
* **Bit Calculation:**
    * **Index:** $log_2(\text{전체 캐시 크기} / \text{캐시라인 크기})$
    * **Offset:** $log_2(\text{캐시라인 크기})$
    * **Tag:** 전체 주소 비트 - (Index + Offset)

### 2. Fully Associative Cache
물리 메모리의 어떤 데이터도 캐시 내의 어떤 라인에든 들어갈 수 있다.
* **Characteristics:** 충돌이 거의 없어 히트율이 높다. 하지만 원하는 데이터를 찾으려면 모든 캐시라인의 태그를 병렬적으로 탐색해야 하므로 캐시라인 개수만큼의 Comparator가 필요하다. 전력 소모가 크고 회로가 복잡하다.
* **Bit Calculation:**
    * **Index:** 사용되지 않음.
    * **Offset:** $log_2(\text{캐시라인 크기})$
    * **Tag:** 전체 주소 비트 - Offset

### 3. Set Associative Cache
직접 사상과 완전 연관 사상의 트레이드오프 방식이다.
* **Characteristics:** 전체 캐시는 세트 단위로 나뉜다. 특정 주소는 정해진 세트에만 들어갈 수 있지만, 세트 내의 여러 라인 중 하나를 자유롭게 선택할 수 있다. 직접 사상보다 충돌이 적고 완전 연관 사상보다 하드웨어 비용이 적다. 현대 CPU 캐시의 표준 방식이다.
* **Bit Calculation:**
    * **Index:** $log_2(\text{전체 캐시 크기} / \text{캐시라인 크기} / \text{세트당 캐시라인 개수})$
    * **Offset:** $log_2(\text{캐시라인 크기})$
    * **Tag:** 전체 주소 비트 - (Index + Offset)


## Cache Miss Types
---
캐시 미스는 발생 원인에 따라 세 가지로 분류된다.

* **Compulsory Miss (Cold Miss):** 프로그램이 처음 실행될 때 캐시가 비어있어 발생하는 불가피한 미스.
* **Capacity Miss:** 캐시 용량이 부족해 발생하는 미스. 워킹 셋이 캐시보다 크면 자주 발생한다.
* **Conflict Miss:** Direct Mapped/Set Associative 방식에서 같은 인덱스를 가진 데이터들이 충돌하여 발생하는 미스.


## Cache Inclusion Policy
---
캐시 계층 간 데이터 중복 저장 정책을 결정한다.

* **Inclusive Cache:** L3가 L1/L2의 모든 데이터를 포함한다. L3만 확인하면 데이터의 존재 여부를 파악할 수 있지만, L3 공간이 중복 데이터로 낭비된다. (Intel의 전통적 방식)
* **Exclusive Cache:** 각 레벨이 서로 다른 데이터만 저장한다. 전체 유효 캐시 용량이 늘어나지만, 데이터 위치 추적을 위한 디렉토리가 필요하다. (AMD의 방식)
* **Non-Inclusive Non-Exclusive (NINE):** L3가 L1/L2 데이터를 포함할 수도, 아닐 수도 있다. Inclusive보다 공간 효율적이고 Exclusive보다 유연하지만 디렉토리 관리가 필요하다. (최근 Intel의 방식)


## Replacement Policy
---
캐시 세트가 가득 찼을 때 어떤 라인을 비울지 결정하는 알고리즘이다.
* **LRU (Least Recently Used):** 가장 오랫동안 사용되지 않은 라인을 교체한다. 이론적으로 우수하지만 구현 비용이 크다.
* **Pseudo-LRU:** 트리나 비트를 이용해 LRU와 유사하게 동작하도록 만든 효율적인 방식이다. 실제 현대 CPU에서 주로 사용한다.
* **Random:** 무작위로 교체하며 구현이 단순하다.


## Write Policy
---
수정된 데이터를 메인 메모리에 반영하는 시점을 결정한다.
* **Write-through:** 캐시와 메모리를 동시에 업데이트한다. 데이터 일관성이 보장되나 성능이 느리다.
* **Write-back:** 캐시만 먼저 수정하고, 해당 라인이 교체될 때 메모리에 기록한다. 다만 멀티코어 환경에서는 코어 간 데이터 불일치 문제를 야기하므로 별도의 일관성 프로토콜이 필요하다.



## Cache Coherence
---
현대 시스템은 성능을 위해 Write-back 쓰기 정책을 주로 사용한다. 이 방식은 캐시의 데이터가 수정되어도 즉시 메모리에 반영하지 않으므로, 특정 시점에 캐시와 메모리의 데이터가 서로 다를 수 있다.

멀티 코어 환경에서는 각 코어가 자신만의 L1, L2 캐시를 가진다. 만약 코어 A가 데이터를 수정했는데 코어 B가 메모리나 자신의 캐시에서 옛날 데이터를 읽어간다면 데이터 불일치 문제가 발생한다. 이를 방지하기 위해 각 코어의 캐시라인을 동기화하는 캐시 일관성 메커니즘이 필요하다.




## Coherence Strategies
---
캐시 일관성을 유지하기 위한 하드웨어적 설계 방식은 크게 두 가지로 나뉜다.

### 1. Snooping
모든 코어가 하나의 공유 버스에 연결되어 있는 구조다.
* **Mechanism:** 각 코어의 캐시 컨트롤러는 버스를 항상 감시한다. 다른 코어가 데이터를 요청하거나 상태를 변경하는 메시지를 버스에 던지면(Broadcast), 이를 가로채서 자신의 캐시 상태를 업데이트한다. 
* **Trade-off:** 설계가 비교적 간단하지만, 코어 수가 많아지면 버스에 트래픽이 몰려 성능이 급격히 저하된다. 즉, 확장성이 떨어진다.

### 2. Directory-based
캐시 라인의 상태 정보를 별도의 중앙 관리자(Directory)에 기록하는 방식이다.
* **Mechanism:** 데이터를 변경하거나 요청하려는 코어는 먼저 디렉토리에 알린다. 디렉토리는 해당 데이터를 공유하고 있는 코어들에게만 핀포인트로 메시지를 보낸다(Unicast/Multicast). 
* **Trade-off**
    * **Pros:** 불필요한 브로드캐스트가 없어 코어가 수백 개 이상인 대규모 시스템에서도 잘 작동한다.
    * **Cons:** 디렉토리 정보를 저장할 별도의 메모리가 필요하고, 매번 관리자를 거쳐야 하므로 통신 단계가 많아져 레이턴시가 발생한다.




## MESI Protocol
---
스누핑 기반 시스템에서 가장 널리 쓰이는 일관성 프로토콜이다. 캐시라인의 상태를 4가지로 정의한다.

* **M (Modified):** 데이터가 수정된 상태. 나만 들고 있으며 메인 메모리와 데이터가 다르다.
* **E (Exclusive):** 나만 들고 있는 상태. 메인 메모리와 데이터가 일치한다.
* **S (Shared):** 다른 코어도 복사본을 들고 있는 상태. 메인 메모리와 데이터가 일치한다.
* **I (Invalid):** 유효하지 않은 데이터. 쓰레기 값이다. 다시 읽어와야 한다.


### Bus Transactions
MESI 상태 전이를 유도하는 주요 버스 트랜잭션은 다음과 같다.

* **BusRd (Bus Read):** 캐시라인에 데이터가 없을 때 데이터를 읽기 위해 요청하는 신호. 다른 코어가 데이터를 가지고 있지 않으면 메모리에서 가져와 E 상태가 되고, 누군가 이미 가지고 있다면 데이터를 공유받아 S 상태가 된다.
* **BusRdX (Bus Read Exclusive):** 캐시라인에 데이터가 없을 때 데이터를 수정하기 위해 발생하는 신호. 캐시라인의 나머지 데이터를 보존해야 하므로 기존 데이터를 가져옴과 동시에 타 코어의 캐시를 무효화(I)하며 자신은 즉시 M 상태가 된다.
* **BusUpgr (Bus Upgrade):** 캐시라인에 데이터가 있고 공유(S) 상태일 때 데이터를 수정하기 위해 발생하는 신호. 데이터 전송 과정 없이 다른 코어들의 캐시만 무효화(I)하여 수정 권한을 획득하고 M 상태가 된다. (단, E 상태일 때는 이미 나만 데이터를 보유한 상태이므로 버스 신호 없이 즉시 M으로 전이한다.)
* **Flush:** M 상태의 코어가 데이터를 버스에 실어 보내는 신호. 메인 메모리를 최신본으로 업데이트함과 동시에 데이터를 요청한 다른 코어에게 전달하며, 이후 자신의 상태는 S로 전이된다.



## Store Buffer
---
실제로는 CPU와 L1 캐시 사이에 Store Buffer라는 작은 버퍼가 존재한다. 메모리 쓰기 연산 시 캐시라인에 직접 쓰는 대신 이 버퍼에 먼저 기록한다.

### Motivation
캐시라인에 직접 쓰기를 시도하면 파이프라인 스톨이 발생한다. 특히 쓰려는 데이터가 캐시에 없거나 공유(S) 상태일 때 문제가 심각하다.

**Store Buffer 없을 때:**
1. CPU가 메모리 쓰기 명령 실행
2. 캐시 미스 또는 S 상태 → BusRdX/BusUpgr 트랜잭션 발생
3. 트랜잭션 완료까지 **수백 사이클 동안 파이프라인 스톨**
4. 완료 후에야 다음 명령 실행

Store Buffer를 사용하면 CPU는 버퍼에 쓰고 즉시 다음 명령을 실행할 수 있다. 버퍼의 데이터는 백그라운드에서 비동기적으로 캐시에 반영된다.

### Memory Ordering Issues
Store Buffer로 인해 멀티코어 환경에서 메모리 순서 재배치 문제가 발생할 수 있다.
```c
// 초기 상태: x = 0, y = 0

// Core 0
x = 1;
r1 = y;

// Core 1  
y = 1;
r2 = x;

// 결과: r1 == 0 && r2 == 0 가능!
```

두 코어가 각각 쓰기 후 읽기를 수행한다. 각 코어는 자신의 Store를 Store Buffer에 넣고 즉시 다음 Load를 실행한다. 이때 상대방의 Store가 아직 메모리에 반영되지 않았을 수 있으므로, 양쪽 모두 이전 값을 읽을 가능성이 있다. 

이러한 문제를 방지하기 위해 메모리 배리어가 필요하다. 배리어를 만나면 CPU는 Store Buffer의 모든 항목이 캐시에 반영될 때까지 파이프라인을 스톨시킨다.

```c
// 초기 상태: x = 0, y = 0

// Core 0
x = 1;
__asm__ volatile("mfence" ::: "memory");
r1 = y;

// Core 1
y = 1;
__asm__ volatile("mfence" ::: "memory");
r2 = x;

// 이제 r1 == 0 && r2 == 0 불가능
```




## False Sharing
---
멀티 스레드 프로그램에서 서로 다른 두 변수가 우연히 같은 캐시라인에 배치되어 성능이 저하되는 현상이다.

### Causes
두 스레드가 각각 독립적인 변수 A와 B를 수정한다고 가정한다. 두 변수가 같은 캐시라인에 묶여 있다면, MESI 프로토콜에 의해 Cache Ping-pong이라 불리는 다음과 같은 비효율이 발생한다.

1. 스레드 1이 변수 A를 수정하면 캐시라인이 M 상태가 된다.
2. 이때 스레드 2가 변수 B를 수정하려면, 스레드 1의 캐시라인을 무효화하고 데이터를 가져와야 한다.
3. 스레드 1이 다시 A를 쓰려면 또 스레드 2의 캐시를 무효화해야 한다.

실제로는 데이터가 충돌하지 않음에도 불구하고, 캐시라인 단위로 일관성을 관리하기 때문에 마치 하나의 데이터를 두고 싸우는 것처럼 불필요한 트래픽 발생과 메모리 쓰기가 반복된다. 

### Solutions
두 변수가 서로 다른 캐시라인에 놓이도록 강제한다. GCC/Clang 환경에서는 `__attribute__((aligned(64)))` 지시어를 사용하여 변수의 시작 주소를 캐시라인 크기(64B)에 맞춘다.